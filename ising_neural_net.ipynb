{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNet to classify Ising states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the NN inputs\n",
    "TODO: decide which labels to use; return it in generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_temperature(filename):\n",
    "    temperature = filename.split('_')[0].replace('T', '')\n",
    "    return float(temperature)\n",
    "\n",
    "def get_labels(files, Tc):\n",
    "    temperatures = np.array([get_temperature(file) for file in files])\n",
    "    labels = temperatures < Tc # 1 for ferromagnetic state, 0 otherwise\n",
    "    return labels.astype(int)\n",
    "\n",
    "def load_spins(file):\n",
    "    spins = np.load(file)\n",
    "    side = int(np.sqrt(len(spins)))\n",
    "    spins = spins.reshape((side, side))\n",
    "    return spins\n",
    "\n",
    "def generate_batch(data_dir, batch_size, Tc=2.0/np.log(1.0 + np.sqrt(2)), num_classes=2):\n",
    "    labels = np.zeros((batch_size, num_classes))\n",
    "#     files = glob(os.path.join(data_dir, 'T{:.3f}*'.format(temperature)))\n",
    "    files = os.listdir(data_dir)\n",
    "    shuffle(files)\n",
    "    \n",
    "    i = 0\n",
    "    while i+batch_size < len(files):\n",
    "        spins = np.array([load_spins(os.path.join(data_dir, file)) \n",
    "                          for file in files[i: i+batch_size]])\n",
    "        labels = get_labels(files[i: i+batch_size], Tc)\n",
    "        i += batch_size\n",
    "        yield spins, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/square_ising_configs_1600spins/'\n",
    "batch_size = 10\n",
    "trainloader = generate_batch(data_dir, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, a=40, kernel_size=4, n_hidden=2, n_out=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 1, kernel_size)\n",
    "        # h_out = int((h_in + 2*h_padding - h_dilatation*(h_kernel-1) -1) / h_stride + 1)\n",
    "        a_out = a - kernel_size + 1 # n_spins = a**2\n",
    "        self.fc1 = nn.Linear(a_out**2, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv): Conv2d(1, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (fc1): Linear (1369 -> 2)\n",
       "  (fc2): Linear (2 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    print('Training...')\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print('[{:d}, {:5d}] loss: {:.3f}'.format(epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
